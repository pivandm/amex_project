{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from xgboost import XGBClassifier\n",
    "import  xgboost as xgb\n",
    "# import os\n",
    "# import sys\n",
    "import gc\n",
    "# import catboost\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# file = open('cat_cols.txt', 'rb')\n",
    "# cat_cols = pickle.load(file)\n",
    "# file.close()\n",
    "class CONFIG:\n",
    "\n",
    "    \"\"\"\n",
    "    Configuration of training\n",
    "\n",
    "    params: dict = Parameters of Tree Booster\n",
    "\n",
    "    n_folds: int = number of splits for Stratified K-Folds\n",
    "\n",
    "    n_rounds: int = number of boosting iterations\n",
    "\n",
    "    early_stopping: int = stop if there is little to no improvement\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters of Tree Booster\n",
    "    params = {\n",
    "        'eta': 0.04,\n",
    "        'gamma': 0,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'max_delta_step': 2, #suggested by xgb documentation for imbalanced dataset\n",
    "        'max_leaves': 127,\n",
    "        'objective': 'binary:logistic',\n",
    "        'disable_default_eval_metric': 1,\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "    # number of folds\n",
    "    n_folds = 8\n",
    "    n_rounds = 1500\n",
    "    early_stopping = 1000\n",
    "    verbose_eval = 25\n",
    "    def output(self):\n",
    "        print('=' * 60)\n",
    "        print('CONFIGURATION')\n",
    "        print('=' * 60)\n",
    "        print('Tree Booster parameters: ')\n",
    "        for k, v in self.params.items():\n",
    "            print(f'{k}: {v}')\n",
    "        print(f'Number of boosting rounds: {self.n_rounds}')\n",
    "        print(f'Early stopping rounds: {self.early_stopping}')\n",
    "        print(f'Stratified K-Fold number of splits: {self.n_folds}')\n",
    "        print('=' * 60)\n",
    "        print('END')\n",
    "        print('=' * 60)\n",
    "    def save(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            print('=' * 60, file=f)\n",
    "            print('CONFIGURATION', file=f)\n",
    "            print('=' * 60, file=f)\n",
    "            print('Tree Booster parameters: ', file=f)\n",
    "            for k, v in self.params.items():\n",
    "                print(f'{k}: {v}', file=f)\n",
    "            print(f'Number of boosting rounds: {self.n_rounds}', file=f)\n",
    "            print(f'Early stopping rounds: {self.early_stopping}', file=f)\n",
    "            print(f'Stratified K-Fold number of splits: {self.n_folds}', file=f)\n",
    "            print('=' * 60, file=f)\n",
    "            print('END', file=f)\n",
    "            print('=' * 60, file=f)\n",
    "def one_hot(train, test):\n",
    "    \"\"\"\n",
    "    Simple encoding using pandas.DataFrame.get_dummies()\n",
    "\n",
    "    !!! If some values do not appear in both datasets in same column\n",
    "    function will not work correctly. Check that test and train have same columns !!!\n",
    "\n",
    "    :param train: train dataset <- pandas.DataFrame\n",
    "    :param test: test dataset <- pandas.DataFrame\n",
    "    :return: same datasets with one hot encoded categoricals -> tuple[pandas.DataFrame, pandas.DataFrame]\n",
    "    \"\"\"\n",
    "    c_enc = train.columns.to_series().groupby(train.dtypes).groups[np.dtype('object')].tolist()\n",
    "    for col in c_enc:\n",
    "        dummies = pd.get_dummies(train[col], prefix=col, drop_first=False)\n",
    "        train = pd.concat([train, dummies], axis=1)\n",
    "        train.drop(columns=col, inplace=True)\n",
    "        dummies = pd.get_dummies(test[col], prefix=col, drop_first=False)\n",
    "        test = pd.concat([test, dummies], axis=1)\n",
    "        test.drop(columns=col, inplace=True)\n",
    "    gc.collect()\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Amex Kaggle Competition Metric\n",
    "    :param y_true: true labels\n",
    "    :param y_pred: predicted values\n",
    "    :return: metric score -> float\n",
    "    \"\"\"\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "def xgb_amex_metric(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    \"\"\"\n",
    "    Custom Metric for XGBoost\n",
    "    using Amex Metric\n",
    "    :param predt: predicted values <- np.ndarray\n",
    "    :param dtrain: matrix to get labels (true values) from <- xgb.Dmatrix\n",
    "    :return: name of the metric, score -> tuple[str, float]\n",
    "    \"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, predt)\n",
    "\n",
    "def mem_usage_gb(df: pandas.DataFrame, deep: bool):\n",
    "    \"\"\"\n",
    "    Memory usage of DataFrame\n",
    "    :param df: dataset <- pandas.DataFrame\n",
    "    :param deep: parameter of pandas.DataFrame.memory_usage(deep=) <- bool\n",
    "    :return: rounded memory usage in GB -> float\n",
    "    \"\"\"\n",
    "    return round((df.memory_usage(deep=deep).sum()/1073741824), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_dataframes(message: str):\n",
    "    print(message)\n",
    "    file = open('dtype_for_agg_catb.txt', 'rb')\n",
    "    dtypes = pickle.load(file)\n",
    "    file.close()\n",
    "    train = pd.read_csv('prep_catboost_train.csv', dtype=dtypes)\n",
    "    train.drop(columns=['customer_ID'], inplace=True)\n",
    "    test = pd.read_csv('prep_catboost_test.csv', dtype=dtypes)\n",
    "    test.drop(columns=['customer_ID'], inplace=True)\n",
    "    train, test = one_hot(train, test)\n",
    "    idx = train.columns.get_loc('D_64_last_-1')\n",
    "    test.insert(loc=idx, column='D_64_last_-1', value=[0] * len(test))\n",
    "    test['D_64_last_-1'] = test['D_64_last_-1'].astype('uint8')\n",
    "    print('encoded cols in train, test')\n",
    "    display(train.dtypes.loc[train.dtypes == 'uint8'], test.dtypes.loc[test.dtypes == 'uint8'])\n",
    "    deep = True\n",
    "    print('train dataset mem usage:', mem_usage_gb(train, deep), 'GB')\n",
    "    print('test dataset mem usage:', mem_usage_gb(test, deep), 'GB')\n",
    "    labels = pd.read_csv('train_labels.csv', dtype={'target': 'int8'})\n",
    "    labels.drop(columns=['customer_ID'], inplace=True)\n",
    "    labels = np.ravel(labels)\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    for k, v in count_dict.items():\n",
    "        print(f'Number of {k} in train_labels: {v} / {len(labels)}, percentage = {round(v / len(labels), 2) * 100}%')\n",
    "    print('datasets loaded, total mem usage: ', mem_usage_gb(train, deep) + mem_usage_gb(test, deep), 'GB')\n",
    "    del deep, unique, counts, count_dict\n",
    "    gc.collect()\n",
    "    return train, test, labels\n",
    "def get_predictions(train_data, test_data, train_labels):\n",
    "    total_predictions = np.zeros(test_data.shape[0])\n",
    "    CONFIG().output()\n",
    "    print('Start training...')\n",
    "    results = dict()\n",
    "    skf = StratifiedKFold(n_splits=CONFIG.n_folds)\n",
    "    n = 1\n",
    "    for train_index, test_index in skf.split(train_data, train_labels):\n",
    "        print('=' * 60)\n",
    "        print(f'Fold number: {n}')\n",
    "        print('=' * 60)\n",
    "        # Get counts of 0 and 1 in K-Fold labels\n",
    "        unique, counts = np.unique(train_labels[train_index], return_counts=True)\n",
    "        count_dict = dict(zip(unique, counts))\n",
    "        for k, v in count_dict.items():\n",
    "            print(f'Number of {k} in train_labels: {v} / {len(train_labels[train_index])}, percentage = {round(v / len(train_labels[train_index]), 2) * 100}%')\n",
    "        unique, counts = np.unique(train_labels[test_index], return_counts=True)\n",
    "        count_dict = dict(zip(unique, counts))\n",
    "        for k, v in count_dict.items():\n",
    "            print(f'Number of {k} in test(eval)_labels: {v} / {len(train_labels[test_index])}, percentage = {round(v / len(train_labels[test_index]), 2) * 100}%')\n",
    "        print('=' * 60)\n",
    "        del unique, counts, count_dict\n",
    "        gc.collect()\n",
    "        n += 1\n",
    "        d_train = xgb.DMatrix(train_data.iloc[train_index], train_labels[train_index])\n",
    "        d_test = xgb.DMatrix(train_data.iloc[test_index], train_labels[test_index])\n",
    "        model = xgb.train(CONFIG.params,\n",
    "                          dtrain= d_train,\n",
    "                          num_boost_round= CONFIG.n_rounds,\n",
    "                          evals= [(d_train, 'train'), (d_test, 'eval')],\n",
    "                          custom_metric= xgb_amex_metric,\n",
    "                          early_stopping_rounds= CONFIG.early_stopping,\n",
    "                          evals_result= results,\n",
    "                          verbose_eval= CONFIG.verbose_eval,\n",
    "                          )\n",
    "        d_out = xgb.DMatrix(test_data)\n",
    "        fold_out_predictions = model.predict(d_out)\n",
    "        # adjust predictions according to n_folds\n",
    "        total_predictions += fold_out_predictions / CONFIG.n_folds\n",
    "        print('Fold model successfully trained. Predictions saved')\n",
    "    del d_train, d_test\n",
    "    gc.collect()\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    output = pd.DataFrame({'customer_ID': sample.customer_ID, 'prediction': total_predictions})\n",
    "    return output, results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "first\\\n",
    "eta = 0.04\n",
    "n_rounds = 1200\n",
    "early_stopping = 600\n",
    "second\\\n",
    "eta = 0.03\n",
    "n_rounds = 3000\n",
    "early stopping = 1500\n",
    "sub2\n",
    "eta = 0.175\n",
    "nrouns 6000\n",
    "early = 3000\n",
    "sub3\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivan Piiashev trying ML\n",
      "encoded cols in train, test\n"
     ]
    },
    {
     "data": {
      "text/plain": "D_63_last_CL    uint8\nD_63_last_CO    uint8\nD_63_last_CR    uint8\nD_63_last_XL    uint8\nD_63_last_XM    uint8\nD_63_last_XZ    uint8\nD_64_last_-1    uint8\nD_64_last_O     uint8\nD_64_last_R     uint8\nD_64_last_U     uint8\ndtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "D_63_last_CL    uint8\nD_63_last_CO    uint8\nD_63_last_CR    uint8\nD_63_last_XL    uint8\nD_63_last_XM    uint8\nD_63_last_XZ    uint8\nD_64_last_-1    uint8\nD_64_last_O     uint8\nD_64_last_R     uint8\nD_64_last_U     uint8\ndtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset mem usage: 0.79 GB\n",
      "test dataset mem usage: 1.59 GB\n",
      "Number of 0 in train_labels: 340085 / 458913, percentage = 74.0%\n",
      "Number of 1 in train_labels: 118828 / 458913, percentage = 26.0%\n",
      "datasets loaded, total mem usage:  2.38 GB\n"
     ]
    }
   ],
   "source": [
    "train_d, test_d, train_l = load_dataframes('Ivan Piiashev trying ML')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION\n",
      "============================================================\n",
      "Tree Booster parameters: \n",
      "eta: 0.04\n",
      "gamma: 0\n",
      "max_depth: 6\n",
      "min_child_weight: 1\n",
      "max_delta_step: 2\n",
      "max_leaves: 127\n",
      "objective: binary:logistic\n",
      "disable_default_eval_metric: 1\n",
      "tree_method: gpu_hist\n",
      "Number of boosting rounds: 1500\n",
      "Early stopping rounds: 1000\n",
      "Stratified K-Fold number of splits: 8\n",
      "============================================================\n",
      "END\n",
      "============================================================\n",
      "Start training...\n",
      "============================================================\n",
      "Fold number: 1\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297574 / 401548, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103974 / 401548, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42511 / 57365, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14854 / 57365, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70732\teval-amex_metric:0.70231\n",
      "[25]\ttrain-amex_metric:0.75283\teval-amex_metric:0.74532\n",
      "[50]\ttrain-amex_metric:0.76575\teval-amex_metric:0.75504\n",
      "[75]\ttrain-amex_metric:0.77548\teval-amex_metric:0.76280\n",
      "[100]\ttrain-amex_metric:0.78245\teval-amex_metric:0.76849\n",
      "[125]\ttrain-amex_metric:0.78906\teval-amex_metric:0.77226\n",
      "[150]\ttrain-amex_metric:0.79574\teval-amex_metric:0.77629\n",
      "[175]\ttrain-amex_metric:0.80138\teval-amex_metric:0.77991\n",
      "[200]\ttrain-amex_metric:0.80593\teval-amex_metric:0.78282\n",
      "[225]\ttrain-amex_metric:0.81001\teval-amex_metric:0.78423\n",
      "[250]\ttrain-amex_metric:0.81396\teval-amex_metric:0.78498\n",
      "[275]\ttrain-amex_metric:0.81755\teval-amex_metric:0.78526\n",
      "[300]\ttrain-amex_metric:0.82139\teval-amex_metric:0.78566\n",
      "[325]\ttrain-amex_metric:0.82478\teval-amex_metric:0.78668\n",
      "[350]\ttrain-amex_metric:0.82796\teval-amex_metric:0.78686\n",
      "[375]\ttrain-amex_metric:0.83084\teval-amex_metric:0.78700\n",
      "[400]\ttrain-amex_metric:0.83369\teval-amex_metric:0.78709\n",
      "[425]\ttrain-amex_metric:0.83708\teval-amex_metric:0.78761\n",
      "[450]\ttrain-amex_metric:0.83979\teval-amex_metric:0.78804\n",
      "[475]\ttrain-amex_metric:0.84249\teval-amex_metric:0.78777\n",
      "[500]\ttrain-amex_metric:0.84562\teval-amex_metric:0.78791\n",
      "[525]\ttrain-amex_metric:0.84826\teval-amex_metric:0.78770\n",
      "[550]\ttrain-amex_metric:0.85075\teval-amex_metric:0.78829\n",
      "[575]\ttrain-amex_metric:0.85331\teval-amex_metric:0.78749\n",
      "[600]\ttrain-amex_metric:0.85588\teval-amex_metric:0.78826\n",
      "[625]\ttrain-amex_metric:0.85824\teval-amex_metric:0.78812\n",
      "[650]\ttrain-amex_metric:0.86072\teval-amex_metric:0.78768\n",
      "[675]\ttrain-amex_metric:0.86287\teval-amex_metric:0.78847\n",
      "[700]\ttrain-amex_metric:0.86573\teval-amex_metric:0.78877\n",
      "[725]\ttrain-amex_metric:0.86786\teval-amex_metric:0.78905\n",
      "[750]\ttrain-amex_metric:0.87029\teval-amex_metric:0.78903\n",
      "[775]\ttrain-amex_metric:0.87274\teval-amex_metric:0.78900\n",
      "[800]\ttrain-amex_metric:0.87526\teval-amex_metric:0.78831\n",
      "[825]\ttrain-amex_metric:0.87770\teval-amex_metric:0.78866\n",
      "[850]\ttrain-amex_metric:0.87973\teval-amex_metric:0.78903\n",
      "[875]\ttrain-amex_metric:0.88167\teval-amex_metric:0.78885\n",
      "[900]\ttrain-amex_metric:0.88388\teval-amex_metric:0.78900\n",
      "[925]\ttrain-amex_metric:0.88586\teval-amex_metric:0.78773\n",
      "[950]\ttrain-amex_metric:0.88835\teval-amex_metric:0.78785\n",
      "[975]\ttrain-amex_metric:0.89058\teval-amex_metric:0.78863\n",
      "[999]\ttrain-amex_metric:0.89222\teval-amex_metric:0.78856\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 2\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297574 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103975 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42511 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14853 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70915\teval-amex_metric:0.70394\n",
      "[25]\ttrain-amex_metric:0.75301\teval-amex_metric:0.74402\n",
      "[50]\ttrain-amex_metric:0.76627\teval-amex_metric:0.75478\n",
      "[75]\ttrain-amex_metric:0.77470\teval-amex_metric:0.76265\n",
      "[100]\ttrain-amex_metric:0.78197\teval-amex_metric:0.76745\n",
      "[125]\ttrain-amex_metric:0.78877\teval-amex_metric:0.77276\n",
      "[150]\ttrain-amex_metric:0.79575\teval-amex_metric:0.77645\n",
      "[175]\ttrain-amex_metric:0.80133\teval-amex_metric:0.78018\n",
      "[200]\ttrain-amex_metric:0.80596\teval-amex_metric:0.78213\n",
      "[225]\ttrain-amex_metric:0.81009\teval-amex_metric:0.78353\n",
      "[250]\ttrain-amex_metric:0.81440\teval-amex_metric:0.78419\n",
      "[275]\ttrain-amex_metric:0.81830\teval-amex_metric:0.78571\n",
      "[300]\ttrain-amex_metric:0.82200\teval-amex_metric:0.78643\n",
      "[325]\ttrain-amex_metric:0.82538\teval-amex_metric:0.78746\n",
      "[350]\ttrain-amex_metric:0.82860\teval-amex_metric:0.78730\n",
      "[375]\ttrain-amex_metric:0.83140\teval-amex_metric:0.78832\n",
      "[400]\ttrain-amex_metric:0.83447\teval-amex_metric:0.78867\n",
      "[425]\ttrain-amex_metric:0.83711\teval-amex_metric:0.78939\n",
      "[450]\ttrain-amex_metric:0.83991\teval-amex_metric:0.79006\n",
      "[475]\ttrain-amex_metric:0.84276\teval-amex_metric:0.79074\n",
      "[500]\ttrain-amex_metric:0.84552\teval-amex_metric:0.79107\n",
      "[525]\ttrain-amex_metric:0.84829\teval-amex_metric:0.79053\n",
      "[550]\ttrain-amex_metric:0.85080\teval-amex_metric:0.79114\n",
      "[575]\ttrain-amex_metric:0.85344\teval-amex_metric:0.79118\n",
      "[600]\ttrain-amex_metric:0.85569\teval-amex_metric:0.79188\n",
      "[625]\ttrain-amex_metric:0.85815\teval-amex_metric:0.79258\n",
      "[650]\ttrain-amex_metric:0.86050\teval-amex_metric:0.79257\n",
      "[675]\ttrain-amex_metric:0.86238\teval-amex_metric:0.79141\n",
      "[700]\ttrain-amex_metric:0.86471\teval-amex_metric:0.79262\n",
      "[725]\ttrain-amex_metric:0.86694\teval-amex_metric:0.79244\n",
      "[750]\ttrain-amex_metric:0.86924\teval-amex_metric:0.79286\n",
      "[775]\ttrain-amex_metric:0.87166\teval-amex_metric:0.79257\n",
      "[800]\ttrain-amex_metric:0.87408\teval-amex_metric:0.79294\n",
      "[825]\ttrain-amex_metric:0.87627\teval-amex_metric:0.79289\n",
      "[850]\ttrain-amex_metric:0.87837\teval-amex_metric:0.79270\n",
      "[875]\ttrain-amex_metric:0.88051\teval-amex_metric:0.79312\n",
      "[900]\ttrain-amex_metric:0.88283\teval-amex_metric:0.79311\n",
      "[925]\ttrain-amex_metric:0.88507\teval-amex_metric:0.79341\n",
      "[950]\ttrain-amex_metric:0.88725\teval-amex_metric:0.79297\n",
      "[975]\ttrain-amex_metric:0.88911\teval-amex_metric:0.79267\n",
      "[999]\ttrain-amex_metric:0.89131\teval-amex_metric:0.79274\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 3\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297574 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103975 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42511 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14853 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70966\teval-amex_metric:0.69785\n",
      "[25]\ttrain-amex_metric:0.75363\teval-amex_metric:0.74386\n",
      "[50]\ttrain-amex_metric:0.76557\teval-amex_metric:0.75309\n",
      "[75]\ttrain-amex_metric:0.77505\teval-amex_metric:0.75918\n",
      "[100]\ttrain-amex_metric:0.78208\teval-amex_metric:0.76575\n",
      "[125]\ttrain-amex_metric:0.78840\teval-amex_metric:0.76984\n",
      "[150]\ttrain-amex_metric:0.79546\teval-amex_metric:0.77332\n",
      "[175]\ttrain-amex_metric:0.80092\teval-amex_metric:0.77672\n",
      "[200]\ttrain-amex_metric:0.80605\teval-amex_metric:0.77878\n",
      "[225]\ttrain-amex_metric:0.81013\teval-amex_metric:0.78152\n",
      "[250]\ttrain-amex_metric:0.81393\teval-amex_metric:0.78236\n",
      "[275]\ttrain-amex_metric:0.81766\teval-amex_metric:0.78300\n",
      "[300]\ttrain-amex_metric:0.82090\teval-amex_metric:0.78334\n",
      "[325]\ttrain-amex_metric:0.82437\teval-amex_metric:0.78456\n",
      "[350]\ttrain-amex_metric:0.82749\teval-amex_metric:0.78526\n",
      "[375]\ttrain-amex_metric:0.83052\teval-amex_metric:0.78565\n",
      "[400]\ttrain-amex_metric:0.83373\teval-amex_metric:0.78561\n",
      "[425]\ttrain-amex_metric:0.83708\teval-amex_metric:0.78518\n",
      "[450]\ttrain-amex_metric:0.83985\teval-amex_metric:0.78608\n",
      "[475]\ttrain-amex_metric:0.84257\teval-amex_metric:0.78597\n",
      "[500]\ttrain-amex_metric:0.84557\teval-amex_metric:0.78548\n",
      "[525]\ttrain-amex_metric:0.84824\teval-amex_metric:0.78621\n",
      "[550]\ttrain-amex_metric:0.85110\teval-amex_metric:0.78659\n",
      "[575]\ttrain-amex_metric:0.85339\teval-amex_metric:0.78597\n",
      "[600]\ttrain-amex_metric:0.85590\teval-amex_metric:0.78627\n",
      "[625]\ttrain-amex_metric:0.85826\teval-amex_metric:0.78624\n",
      "[650]\ttrain-amex_metric:0.86056\teval-amex_metric:0.78632\n",
      "[675]\ttrain-amex_metric:0.86322\teval-amex_metric:0.78705\n",
      "[700]\ttrain-amex_metric:0.86543\teval-amex_metric:0.78665\n",
      "[725]\ttrain-amex_metric:0.86796\teval-amex_metric:0.78646\n",
      "[750]\ttrain-amex_metric:0.87025\teval-amex_metric:0.78637\n",
      "[775]\ttrain-amex_metric:0.87229\teval-amex_metric:0.78695\n",
      "[800]\ttrain-amex_metric:0.87470\teval-amex_metric:0.78617\n",
      "[825]\ttrain-amex_metric:0.87708\teval-amex_metric:0.78632\n",
      "[850]\ttrain-amex_metric:0.87910\teval-amex_metric:0.78639\n",
      "[875]\ttrain-amex_metric:0.88142\teval-amex_metric:0.78634\n",
      "[900]\ttrain-amex_metric:0.88375\teval-amex_metric:0.78602\n",
      "[925]\ttrain-amex_metric:0.88561\teval-amex_metric:0.78587\n",
      "[950]\ttrain-amex_metric:0.88761\teval-amex_metric:0.78634\n",
      "[975]\ttrain-amex_metric:0.88947\teval-amex_metric:0.78630\n",
      "[1000]\ttrain-amex_metric:0.89186\teval-amex_metric:0.78633\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 4\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297574 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103975 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42511 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14853 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70971\teval-amex_metric:0.70337\n",
      "[25]\ttrain-amex_metric:0.75249\teval-amex_metric:0.74740\n",
      "[50]\ttrain-amex_metric:0.76538\teval-amex_metric:0.75693\n",
      "[75]\ttrain-amex_metric:0.77507\teval-amex_metric:0.76444\n",
      "[100]\ttrain-amex_metric:0.78198\teval-amex_metric:0.77065\n",
      "[125]\ttrain-amex_metric:0.78863\teval-amex_metric:0.77479\n",
      "[150]\ttrain-amex_metric:0.79548\teval-amex_metric:0.77926\n",
      "[175]\ttrain-amex_metric:0.80100\teval-amex_metric:0.78322\n",
      "[200]\ttrain-amex_metric:0.80581\teval-amex_metric:0.78367\n",
      "[225]\ttrain-amex_metric:0.81067\teval-amex_metric:0.78678\n",
      "[250]\ttrain-amex_metric:0.81478\teval-amex_metric:0.78714\n",
      "[275]\ttrain-amex_metric:0.81843\teval-amex_metric:0.78749\n",
      "[300]\ttrain-amex_metric:0.82174\teval-amex_metric:0.78796\n",
      "[325]\ttrain-amex_metric:0.82532\teval-amex_metric:0.78886\n",
      "[350]\ttrain-amex_metric:0.82877\teval-amex_metric:0.78884\n",
      "[375]\ttrain-amex_metric:0.83137\teval-amex_metric:0.78945\n",
      "[400]\ttrain-amex_metric:0.83422\teval-amex_metric:0.78943\n",
      "[425]\ttrain-amex_metric:0.83711\teval-amex_metric:0.78932\n",
      "[450]\ttrain-amex_metric:0.84014\teval-amex_metric:0.78976\n",
      "[475]\ttrain-amex_metric:0.84279\teval-amex_metric:0.79036\n",
      "[500]\ttrain-amex_metric:0.84572\teval-amex_metric:0.79073\n",
      "[525]\ttrain-amex_metric:0.84806\teval-amex_metric:0.79096\n",
      "[550]\ttrain-amex_metric:0.85023\teval-amex_metric:0.79050\n",
      "[575]\ttrain-amex_metric:0.85261\teval-amex_metric:0.79048\n",
      "[600]\ttrain-amex_metric:0.85508\teval-amex_metric:0.79040\n",
      "[625]\ttrain-amex_metric:0.85754\teval-amex_metric:0.79027\n",
      "[650]\ttrain-amex_metric:0.86003\teval-amex_metric:0.78988\n",
      "[675]\ttrain-amex_metric:0.86287\teval-amex_metric:0.78935\n",
      "[700]\ttrain-amex_metric:0.86531\teval-amex_metric:0.78952\n",
      "[725]\ttrain-amex_metric:0.86735\teval-amex_metric:0.78949\n",
      "[750]\ttrain-amex_metric:0.86988\teval-amex_metric:0.78895\n",
      "[775]\ttrain-amex_metric:0.87236\teval-amex_metric:0.78861\n",
      "[800]\ttrain-amex_metric:0.87438\teval-amex_metric:0.78936\n",
      "[825]\ttrain-amex_metric:0.87686\teval-amex_metric:0.78908\n",
      "[850]\ttrain-amex_metric:0.87899\teval-amex_metric:0.78950\n",
      "[875]\ttrain-amex_metric:0.88160\teval-amex_metric:0.78959\n",
      "[900]\ttrain-amex_metric:0.88410\teval-amex_metric:0.78949\n",
      "[925]\ttrain-amex_metric:0.88625\teval-amex_metric:0.78905\n",
      "[950]\ttrain-amex_metric:0.88855\teval-amex_metric:0.78898\n",
      "[975]\ttrain-amex_metric:0.89054\teval-amex_metric:0.78856\n",
      "[999]\ttrain-amex_metric:0.89238\teval-amex_metric:0.78828\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 5\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297574 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103975 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42511 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14853 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70590\teval-amex_metric:0.70339\n",
      "[25]\ttrain-amex_metric:0.75294\teval-amex_metric:0.74689\n",
      "[50]\ttrain-amex_metric:0.76581\teval-amex_metric:0.75692\n",
      "[75]\ttrain-amex_metric:0.77447\teval-amex_metric:0.76551\n",
      "[100]\ttrain-amex_metric:0.78162\teval-amex_metric:0.77079\n",
      "[125]\ttrain-amex_metric:0.78816\teval-amex_metric:0.77511\n",
      "[150]\ttrain-amex_metric:0.79460\teval-amex_metric:0.77782\n",
      "[175]\ttrain-amex_metric:0.80039\teval-amex_metric:0.78011\n",
      "[200]\ttrain-amex_metric:0.80571\teval-amex_metric:0.78064\n",
      "[225]\ttrain-amex_metric:0.81021\teval-amex_metric:0.78238\n",
      "[250]\ttrain-amex_metric:0.81436\teval-amex_metric:0.78341\n",
      "[275]\ttrain-amex_metric:0.81769\teval-amex_metric:0.78466\n",
      "[300]\ttrain-amex_metric:0.82122\teval-amex_metric:0.78527\n",
      "[325]\ttrain-amex_metric:0.82456\teval-amex_metric:0.78659\n",
      "[350]\ttrain-amex_metric:0.82753\teval-amex_metric:0.78776\n",
      "[375]\ttrain-amex_metric:0.83020\teval-amex_metric:0.78749\n",
      "[400]\ttrain-amex_metric:0.83312\teval-amex_metric:0.78806\n",
      "[425]\ttrain-amex_metric:0.83603\teval-amex_metric:0.78845\n",
      "[450]\ttrain-amex_metric:0.83889\teval-amex_metric:0.78796\n",
      "[475]\ttrain-amex_metric:0.84138\teval-amex_metric:0.78884\n",
      "[500]\ttrain-amex_metric:0.84434\teval-amex_metric:0.78899\n",
      "[525]\ttrain-amex_metric:0.84673\teval-amex_metric:0.78869\n",
      "[550]\ttrain-amex_metric:0.84939\teval-amex_metric:0.78843\n",
      "[575]\ttrain-amex_metric:0.85173\teval-amex_metric:0.78870\n",
      "[600]\ttrain-amex_metric:0.85437\teval-amex_metric:0.78921\n",
      "[625]\ttrain-amex_metric:0.85719\teval-amex_metric:0.78917\n",
      "[650]\ttrain-amex_metric:0.85999\teval-amex_metric:0.78939\n",
      "[675]\ttrain-amex_metric:0.86192\teval-amex_metric:0.78984\n",
      "[700]\ttrain-amex_metric:0.86442\teval-amex_metric:0.78983\n",
      "[725]\ttrain-amex_metric:0.86669\teval-amex_metric:0.78898\n",
      "[750]\ttrain-amex_metric:0.86915\teval-amex_metric:0.78919\n",
      "[775]\ttrain-amex_metric:0.87147\teval-amex_metric:0.78927\n",
      "[800]\ttrain-amex_metric:0.87371\teval-amex_metric:0.78873\n",
      "[825]\ttrain-amex_metric:0.87575\teval-amex_metric:0.78870\n",
      "[850]\ttrain-amex_metric:0.87786\teval-amex_metric:0.78924\n",
      "[875]\ttrain-amex_metric:0.88055\teval-amex_metric:0.78887\n",
      "[900]\ttrain-amex_metric:0.88295\teval-amex_metric:0.78907\n",
      "[925]\ttrain-amex_metric:0.88505\teval-amex_metric:0.78913\n",
      "[950]\ttrain-amex_metric:0.88728\teval-amex_metric:0.78864\n",
      "[975]\ttrain-amex_metric:0.88949\teval-amex_metric:0.78927\n",
      "[1000]\ttrain-amex_metric:0.89149\teval-amex_metric:0.78860\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 6\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297575 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103974 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42510 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14854 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70867\teval-amex_metric:0.70207\n",
      "[25]\ttrain-amex_metric:0.75265\teval-amex_metric:0.74777\n",
      "[50]\ttrain-amex_metric:0.76558\teval-amex_metric:0.76008\n",
      "[75]\ttrain-amex_metric:0.77514\teval-amex_metric:0.76645\n",
      "[100]\ttrain-amex_metric:0.78199\teval-amex_metric:0.77135\n",
      "[125]\ttrain-amex_metric:0.78868\teval-amex_metric:0.77461\n",
      "[150]\ttrain-amex_metric:0.79536\teval-amex_metric:0.77600\n",
      "[175]\ttrain-amex_metric:0.80088\teval-amex_metric:0.77883\n",
      "[200]\ttrain-amex_metric:0.80592\teval-amex_metric:0.78119\n",
      "[225]\ttrain-amex_metric:0.81047\teval-amex_metric:0.78306\n",
      "[250]\ttrain-amex_metric:0.81474\teval-amex_metric:0.78357\n",
      "[275]\ttrain-amex_metric:0.81841\teval-amex_metric:0.78359\n",
      "[300]\ttrain-amex_metric:0.82146\teval-amex_metric:0.78532\n",
      "[325]\ttrain-amex_metric:0.82462\teval-amex_metric:0.78592\n",
      "[350]\ttrain-amex_metric:0.82815\teval-amex_metric:0.78741\n",
      "[375]\ttrain-amex_metric:0.83108\teval-amex_metric:0.78780\n",
      "[400]\ttrain-amex_metric:0.83420\teval-amex_metric:0.78751\n",
      "[425]\ttrain-amex_metric:0.83675\teval-amex_metric:0.78817\n",
      "[450]\ttrain-amex_metric:0.83945\teval-amex_metric:0.78854\n",
      "[475]\ttrain-amex_metric:0.84224\teval-amex_metric:0.79015\n",
      "[500]\ttrain-amex_metric:0.84492\teval-amex_metric:0.78963\n",
      "[525]\ttrain-amex_metric:0.84799\teval-amex_metric:0.78973\n",
      "[550]\ttrain-amex_metric:0.85071\teval-amex_metric:0.79080\n",
      "[575]\ttrain-amex_metric:0.85298\teval-amex_metric:0.79060\n",
      "[600]\ttrain-amex_metric:0.85568\teval-amex_metric:0.79009\n",
      "[625]\ttrain-amex_metric:0.85769\teval-amex_metric:0.79015\n",
      "[650]\ttrain-amex_metric:0.85960\teval-amex_metric:0.79022\n",
      "[675]\ttrain-amex_metric:0.86200\teval-amex_metric:0.79026\n",
      "[700]\ttrain-amex_metric:0.86455\teval-amex_metric:0.78987\n",
      "[725]\ttrain-amex_metric:0.86696\teval-amex_metric:0.79042\n",
      "[750]\ttrain-amex_metric:0.86957\teval-amex_metric:0.79103\n",
      "[775]\ttrain-amex_metric:0.87192\teval-amex_metric:0.79114\n",
      "[800]\ttrain-amex_metric:0.87418\teval-amex_metric:0.79212\n",
      "[825]\ttrain-amex_metric:0.87654\teval-amex_metric:0.79154\n",
      "[850]\ttrain-amex_metric:0.87852\teval-amex_metric:0.79129\n",
      "[875]\ttrain-amex_metric:0.88069\teval-amex_metric:0.79078\n",
      "[900]\ttrain-amex_metric:0.88270\teval-amex_metric:0.79122\n",
      "[925]\ttrain-amex_metric:0.88496\teval-amex_metric:0.79079\n",
      "[950]\ttrain-amex_metric:0.88730\teval-amex_metric:0.79083\n",
      "[975]\ttrain-amex_metric:0.88957\teval-amex_metric:0.79054\n",
      "[1000]\ttrain-amex_metric:0.89160\teval-amex_metric:0.79112\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 7\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297575 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103974 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42510 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14854 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70658\teval-amex_metric:0.69839\n",
      "[25]\ttrain-amex_metric:0.75197\teval-amex_metric:0.74796\n",
      "[50]\ttrain-amex_metric:0.76496\teval-amex_metric:0.75982\n",
      "[75]\ttrain-amex_metric:0.77429\teval-amex_metric:0.76936\n",
      "[100]\ttrain-amex_metric:0.78116\teval-amex_metric:0.77486\n",
      "[125]\ttrain-amex_metric:0.78769\teval-amex_metric:0.77806\n",
      "[150]\ttrain-amex_metric:0.79455\teval-amex_metric:0.78274\n",
      "[175]\ttrain-amex_metric:0.80028\teval-amex_metric:0.78418\n",
      "[200]\ttrain-amex_metric:0.80530\teval-amex_metric:0.78551\n",
      "[225]\ttrain-amex_metric:0.81013\teval-amex_metric:0.78652\n",
      "[250]\ttrain-amex_metric:0.81371\teval-amex_metric:0.78778\n",
      "[275]\ttrain-amex_metric:0.81751\teval-amex_metric:0.78866\n",
      "[300]\ttrain-amex_metric:0.82089\teval-amex_metric:0.79024\n",
      "[325]\ttrain-amex_metric:0.82460\teval-amex_metric:0.79083\n",
      "[350]\ttrain-amex_metric:0.82808\teval-amex_metric:0.79164\n",
      "[375]\ttrain-amex_metric:0.83094\teval-amex_metric:0.79272\n",
      "[400]\ttrain-amex_metric:0.83376\teval-amex_metric:0.79282\n",
      "[425]\ttrain-amex_metric:0.83640\teval-amex_metric:0.79331\n",
      "[450]\ttrain-amex_metric:0.83966\teval-amex_metric:0.79319\n",
      "[475]\ttrain-amex_metric:0.84227\teval-amex_metric:0.79344\n",
      "[500]\ttrain-amex_metric:0.84522\teval-amex_metric:0.79320\n",
      "[525]\ttrain-amex_metric:0.84785\teval-amex_metric:0.79307\n",
      "[550]\ttrain-amex_metric:0.85063\teval-amex_metric:0.79263\n",
      "[575]\ttrain-amex_metric:0.85364\teval-amex_metric:0.79202\n",
      "[600]\ttrain-amex_metric:0.85659\teval-amex_metric:0.79211\n",
      "[625]\ttrain-amex_metric:0.85901\teval-amex_metric:0.79245\n",
      "[650]\ttrain-amex_metric:0.86118\teval-amex_metric:0.79300\n",
      "[675]\ttrain-amex_metric:0.86334\teval-amex_metric:0.79317\n",
      "[700]\ttrain-amex_metric:0.86569\teval-amex_metric:0.79296\n",
      "[725]\ttrain-amex_metric:0.86806\teval-amex_metric:0.79324\n",
      "[750]\ttrain-amex_metric:0.87032\teval-amex_metric:0.79351\n",
      "[775]\ttrain-amex_metric:0.87243\teval-amex_metric:0.79276\n",
      "[800]\ttrain-amex_metric:0.87462\teval-amex_metric:0.79270\n",
      "[825]\ttrain-amex_metric:0.87694\teval-amex_metric:0.79339\n",
      "[850]\ttrain-amex_metric:0.87922\teval-amex_metric:0.79338\n",
      "[875]\ttrain-amex_metric:0.88124\teval-amex_metric:0.79293\n",
      "[900]\ttrain-amex_metric:0.88384\teval-amex_metric:0.79297\n",
      "[925]\ttrain-amex_metric:0.88594\teval-amex_metric:0.79297\n",
      "[950]\ttrain-amex_metric:0.88832\teval-amex_metric:0.79330\n",
      "[975]\ttrain-amex_metric:0.89062\teval-amex_metric:0.79361\n",
      "[1000]\ttrain-amex_metric:0.89229\teval-amex_metric:0.79345\n",
      "Fold model successfully trained. Predictions saved\n",
      "============================================================\n",
      "Fold number: 8\n",
      "============================================================\n",
      "Number of 0 in train_labels: 297575 / 401549, percentage = 74.0%\n",
      "Number of 1 in train_labels: 103974 / 401549, percentage = 26.0%\n",
      "Number of 0 in test(eval)_labels: 42510 / 57364, percentage = 74.0%\n",
      "Number of 1 in test(eval)_labels: 14854 / 57364, percentage = 26.0%\n",
      "============================================================\n",
      "[0]\ttrain-amex_metric:0.70740\teval-amex_metric:0.70735\n",
      "[25]\ttrain-amex_metric:0.75225\teval-amex_metric:0.74813\n",
      "[50]\ttrain-amex_metric:0.76495\teval-amex_metric:0.75994\n",
      "[75]\ttrain-amex_metric:0.77414\teval-amex_metric:0.76744\n",
      "[100]\ttrain-amex_metric:0.78089\teval-amex_metric:0.77357\n",
      "[125]\ttrain-amex_metric:0.78755\teval-amex_metric:0.77694\n",
      "[150]\ttrain-amex_metric:0.79442\teval-amex_metric:0.78102\n",
      "[175]\ttrain-amex_metric:0.80065\teval-amex_metric:0.78234\n",
      "[200]\ttrain-amex_metric:0.80533\teval-amex_metric:0.78374\n",
      "[225]\ttrain-amex_metric:0.80967\teval-amex_metric:0.78518\n",
      "[250]\ttrain-amex_metric:0.81361\teval-amex_metric:0.78670\n",
      "[275]\ttrain-amex_metric:0.81677\teval-amex_metric:0.78789\n",
      "[300]\ttrain-amex_metric:0.82048\teval-amex_metric:0.78843\n",
      "[325]\ttrain-amex_metric:0.82403\teval-amex_metric:0.78971\n",
      "[350]\ttrain-amex_metric:0.82732\teval-amex_metric:0.78989\n",
      "[375]\ttrain-amex_metric:0.83036\teval-amex_metric:0.79030\n",
      "[400]\ttrain-amex_metric:0.83307\teval-amex_metric:0.79107\n",
      "[425]\ttrain-amex_metric:0.83607\teval-amex_metric:0.79131\n",
      "[450]\ttrain-amex_metric:0.83916\teval-amex_metric:0.79254\n",
      "[475]\ttrain-amex_metric:0.84235\teval-amex_metric:0.79192\n",
      "[500]\ttrain-amex_metric:0.84501\teval-amex_metric:0.79203\n",
      "[525]\ttrain-amex_metric:0.84767\teval-amex_metric:0.79177\n",
      "[550]\ttrain-amex_metric:0.85012\teval-amex_metric:0.79245\n",
      "[575]\ttrain-amex_metric:0.85242\teval-amex_metric:0.79243\n",
      "[600]\ttrain-amex_metric:0.85494\teval-amex_metric:0.79280\n",
      "[625]\ttrain-amex_metric:0.85766\teval-amex_metric:0.79238\n",
      "[650]\ttrain-amex_metric:0.85981\teval-amex_metric:0.79241\n",
      "[675]\ttrain-amex_metric:0.86238\teval-amex_metric:0.79172\n",
      "[700]\ttrain-amex_metric:0.86503\teval-amex_metric:0.79221\n",
      "[725]\ttrain-amex_metric:0.86730\teval-amex_metric:0.79214\n",
      "[750]\ttrain-amex_metric:0.86965\teval-amex_metric:0.79201\n",
      "[775]\ttrain-amex_metric:0.87196\teval-amex_metric:0.79156\n",
      "[800]\ttrain-amex_metric:0.87439\teval-amex_metric:0.79184\n",
      "[825]\ttrain-amex_metric:0.87674\teval-amex_metric:0.79162\n",
      "[850]\ttrain-amex_metric:0.87896\teval-amex_metric:0.79165\n",
      "[875]\ttrain-amex_metric:0.88101\teval-amex_metric:0.79100\n",
      "[900]\ttrain-amex_metric:0.88299\teval-amex_metric:0.79116\n",
      "[925]\ttrain-amex_metric:0.88561\teval-amex_metric:0.79115\n",
      "[950]\ttrain-amex_metric:0.88808\teval-amex_metric:0.79112\n",
      "[975]\ttrain-amex_metric:0.88998\teval-amex_metric:0.79180\n",
      "[999]\ttrain-amex_metric:0.89220\teval-amex_metric:0.79175\n",
      "Fold model successfully trained. Predictions saved\n"
     ]
    }
   ],
   "source": [
    "ofile, result = get_predictions(train_d, test_d, train_l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ofile.to_csv('E:/amex_data_notebooks/subs/sub3_k_folds.csv', index=False)\n",
    "CONFIG().save('E:/amex_data_notebooks/subs/subb3_config.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
