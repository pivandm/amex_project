{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will restrict TensorFlow to max 8GB GPU RAM\n",
      "then RAPIDS can use 8GB GPU RAM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd # CPU LIBRARIES\n",
    "import matplotlib.pyplot as plt, gc\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "LIMIT = 8\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "print('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:06:09.199996Z",
     "start_time": "2023-05-31T14:06:09.092804Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "PATH_TO_CUSTOMER_HASHES = '../input/amex-data-files/'\n",
    "\n",
    "# AFTER PROCESSING DATA ONCE, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "PROCESS_DATA = True\n",
    "PATH_TO_DATA = './data/'\n",
    "PATH_TO_DATA = '../input/amex-data-for-transformers-and-rnns/data/'\n",
    "\n",
    "# AFTER TRAINING MODEL, UPLOAD TO KAGGLE DATASET\n",
    "# THEN SET VARIABLE BELOW TO FALSE\n",
    "# AND ATTACH DATASET TO NOTEBOOK AND PUT PATH TO DATASET BELOW\n",
    "TRAIN_MODEL = True\n",
    "PATH_TO_MODEL = './model/'\n",
    "PATH_TO_MODEL = '../input/amex-data-for-transformers-and-rnns/model/'\n",
    "\n",
    "INFER_TEST = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:08:47.603410Z",
     "start_time": "2023-05-31T14:08:47.595630Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd # CPU LIBRARIES\n",
    "import matplotlib.pyplot as plt, gc\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    # LOAD TARGETS\n",
    "    targets = pd.read_csv('train_data.csv')\n",
    "    targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    print(f'There are {targets.shape[0]} train targets')\n",
    "\n",
    "    # GET TRAIN COLUMN NAMES\n",
    "    train = pd.read_csv('train_data.csv', nrows=1)\n",
    "    T_COLS = train.columns\n",
    "    print(f'There are {len(T_COLS)} train dataframe columns')\n",
    "\n",
    "    # GET TRAIN CUSTOMER NAMES (use pandas to avoid memory error)\n",
    "    if PATH_TO_CUSTOMER_HASHES:\n",
    "        train = pd.read_parquet(f'{PATH_TO_CUSTOMER_HASHES}train_customer_hashes.pqt')\n",
    "    else:\n",
    "        train = pd.read_csv('train_data.csv', usecols=['customer_ID'])\n",
    "        train['customer_ID'] = train['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "    customers = train.drop_duplicates().sort_index().values.flatten()\n",
    "    print(f'There are {len(customers)} unique customers in train.')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-31T14:13:30.159234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_rows(customers, train, NUM_FILES = 10, verbose = ''):\n",
    "    chunk = len(customers)//NUM_FILES\n",
    "    if verbose != '':\n",
    "        print(f'We will split {verbose} data into {NUM_FILES} separate files.')\n",
    "        print(f'There will be {chunk} customers in each file (except the last file).')\n",
    "        print('Below are number of rows in each file:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_FILES):\n",
    "        if k==NUM_FILES-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': print( rows )\n",
    "    return rows\n",
    "    rows = get_rows(customers, train, NUM_FILES = NUM_FILES, verbose = 'train')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T13:35:34.467263Z",
     "start_time": "2023-05-31T13:35:34.451899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n",
    "#\n",
    "#     # REDUCE STRING COLUMNS\n",
    "#     # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "#     train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "#     train.S_2 = cudf.to_datetime( train.S_2 )\n",
    "#     train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "#     train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "#     train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "#     del train['S_2']\n",
    "#\n",
    "#     # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "#     # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "#     d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "#     train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "#\n",
    "#     d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "#     train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "#\n",
    "#     CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "#     OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "#     # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "#     for c,s in zip(CATS,OFFSETS):\n",
    "#         train[c] = train[c] + s\n",
    "#         train[c] = train[c].fillna(1).astype('int8')\n",
    "#     CATS += ['D_63','D_64']\n",
    "#\n",
    "#     # ADD NEW FEATURES HERE\n",
    "#     # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "#     # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "#     # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "#\n",
    "#     # REDUCE MEMORY DTYPE\n",
    "#     SKIP = ['customer_ID','year','month','day']\n",
    "#     for c in train.columns:\n",
    "#         if c in SKIP: continue\n",
    "#         if str( train[c].dtype )=='int64':\n",
    "#             train[c] = train[c].astype('int32')\n",
    "#         if str( train[c].dtype )=='float64':\n",
    "#             train[c] = train[c].astype('float32')\n",
    "#\n",
    "#     # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "#     if PAD_CUSTOMER_TO_13_ROWS:\n",
    "#         tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "#         more = cupy.array([],dtype='int64')\n",
    "#         for j in range(1,13):\n",
    "#             i = tmp.loc[tmp==j].index.values\n",
    "#             more = cupy.concatenate([more,cupy.repeat(i,13-j)])\n",
    "#         df = train.iloc[:len(more)].copy().fillna(0)\n",
    "#         df = df * 0 - 1 #pad numerical columns with -1\n",
    "#         df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "#         df['customer_ID'] = more\n",
    "#         train = cudf.concat([train,df],axis=0,ignore_index=True)\n",
    "#\n",
    "#     # ADD TARGETS (and reduce to 1 byte)\n",
    "#     if targets is not None:\n",
    "#         train = train.merge(targets,on='customer_ID',how='left')\n",
    "#         train.target = train.target.astype('int8')\n",
    "#\n",
    "#     # FILL NAN\n",
    "#     train = train.fillna(-0.5) #this applies to numerical columns\n",
    "#\n",
    "#     # SORT BY CUSTOMER THEN DATE\n",
    "#     train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "#     train = train.drop(['year','month','day'],axis=1)\n",
    "#\n",
    "#     # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "#     COLS = list(train.columns[1:])\n",
    "#     COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "#     train = train[COLS]\n",
    "#\n",
    "#     return train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T12:44:36.973490Z",
     "start_time": "2023-05-31T12:44:36.953828Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
