{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import  xgboost as xgb\n",
    "import os\n",
    "import gc\n",
    "import catboost as ctb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PREPROCESS:\n",
    "    categorical = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    catboost_prep = True\n",
    "    xgb_lgb_prep = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CONFIG:\n",
    "    def __init__(self, name: str, params: dict, num_boost_round: int, early_stopping = None, verbose_eval = None, n_folds = None):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.early_stopping = early_stopping\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.n_folds = n_folds\n",
    "    def __str__(self):\n",
    "        s = self.name; s += '\\n'\n",
    "        s += 'Booster parameters:\\n'\n",
    "        for k, v in self.params.items():\n",
    "            s += f'{k}: {v}\\n'\n",
    "        s += '.train() args:\\n'\n",
    "        s += f'Number of boosting rounds: {self.num_boost_round}\\n'\n",
    "        if self.early_stopping is not None:\n",
    "            s += f'Early stopping rounds: {self.early_stopping}\\n'\n",
    "        if self.verbose_eval is not None:\n",
    "            s += f'Verbose eval: {self.verbose_eval}\\n'\n",
    "        if self.n_folds is  not None:\n",
    "            s += f'Stratified K-Fold number of splits: {self.n_folds}\\n'\n",
    "        s += 'END OF CONFIG'\n",
    "        return s\n",
    "    def save(self, path: str):\n",
    "        f = open(path, 'w')\n",
    "        print(self, file=f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(train, test):\n",
    "    \"\"\"\n",
    "    Simple encoding using pandas.DataFrame.get_dummies()\n",
    "\n",
    "    !!! If some values do not appear in both datasets in same column\n",
    "    function will not work correctly. Check that test and train have same columns !!!\n",
    "\n",
    "    :param train: train dataset <- pandas.DataFrame\n",
    "    :param test: test dataset <- pandas.DataFrame\n",
    "    :return: same datasets with one hot encoded categoricals -> tuple[pandas.DataFrame, pandas.DataFrame]\n",
    "    \"\"\"\n",
    "    c_enc = train.columns.to_series().groupby(train.dtypes).groups[np.dtype('object')].tolist()\n",
    "    for col in c_enc:\n",
    "        dummies = pd.get_dummies(train[col], prefix=col, drop_first=False)\n",
    "        train = pd.concat([train, dummies], axis=1)\n",
    "        train.drop(columns=col, inplace=True)\n",
    "        dummies = pd.get_dummies(test[col], prefix=col, drop_first=False)\n",
    "        test = pd.concat([test, dummies], axis=1)\n",
    "        test.drop(columns=col, inplace=True)\n",
    "    gc.collect()\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Amex Kaggle Competition Metric\n",
    "    :param y_true: true labels\n",
    "    :param y_pred: predicted values\n",
    "    :return: metric score -> float\n",
    "    \"\"\"\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "def lgb_amex_metric(preds: np.ndarray, train_data: lgb.Dataset):\n",
    "    y_true = train_data.get_label()\n",
    "    eval_name = 'amex_metric'\n",
    "    value = amex_metric(y_true, preds)\n",
    "    is_higher_better = True\n",
    "    return eval_name, value, is_higher_better\n",
    "def xgb_amex_metric(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    \"\"\"\n",
    "    Custom Metric for XGBoost\n",
    "    using Amex Metric\n",
    "    :param predt: predicted values <- np.ndarray\n",
    "    :param dtrain: matrix to get labels (true values) from <- xgb.Dmatrix\n",
    "    :return: name of the metric, score -> tuple[str, float]\n",
    "    \"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, predt)\n",
    "def file_mem_usage_gb(file_name: str):\n",
    "    \"\"\"\n",
    "    Disk memory usage of file in GB\n",
    "    :param file_name: pathlike[str]\n",
    "    :return: size in GB -> float\n",
    "    \"\"\"\n",
    "    return round((os.stat(file_name).st_size/1073741824), 2)\n",
    "def mem_usage_gb(df: pandas.DataFrame, deep: bool):\n",
    "    \"\"\"\n",
    "    Memory usage of DataFrame\n",
    "    :param df: dataset <- pandas.DataFrame\n",
    "    :param deep: parameter of pandas.DataFrame.memory_usage(deep=) <- bool\n",
    "    :return: rounded memory usage in GB -> float\n",
    "    \"\"\"\n",
    "    return round((df.memory_usage(deep=deep).sum()/1073741824), 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG_XGB\n",
      "Booster parameters:\n",
      "eta: 0.04\n",
      "gamma: 0\n",
      "max_depth: 6\n",
      "min_child_weight: 1\n",
      "max_delta_step: 2\n",
      "max_leaves: 127\n",
      "objective: binary:logistic\n",
      "disable_default_eval_metric: 1\n",
      "tree_method: gpu_hist\n",
      ".train() args:\n",
      "Number of boosting rounds: 1500\n",
      "Early stopping rounds: 1000\n",
      "Verbose eval: 25\n",
      "Stratified K-Fold number of splits: 8\n",
      "END OF CONFIG\n",
      "CONFIG_LGBM\n",
      "Booster parameters:\n",
      "boosting_type: dart\n",
      "objective: binary\n",
      "learning_rate: 0.04\n",
      "num_leaves: 127\n",
      ".train() args:\n",
      "Number of boosting rounds: 1100\n",
      "END OF CONFIG\n"
     ]
    }
   ],
   "source": [
    "config_xgb = CONFIG('CONFIG_XGB',\n",
    "                    params= {\n",
    "                        'eta': 0.04,\n",
    "                        'gamma': 0,\n",
    "                        'max_depth': 6,\n",
    "                        'min_child_weight': 1,\n",
    "                        'max_delta_step': 2, #suggested by xgb documentation for imbalanced dataset\n",
    "                        'max_leaves': 127,\n",
    "                        'objective': 'binary:logistic',\n",
    "                        'disable_default_eval_metric': 1,\n",
    "                        'tree_method': 'gpu_hist'},\n",
    "                    num_boost_round= 1500,\n",
    "                    early_stopping= 1000,\n",
    "                    verbose_eval= 250,\n",
    "                    n_folds= 5\n",
    "                    )\n",
    "config_lgb = CONFIG('CONFIG_LGBM',\n",
    "                    params= {\n",
    "                        'boosting_type': 'dart',\n",
    "                        'objective': 'binary',\n",
    "                        'learning_rate': 0.04,\n",
    "                        'num_leaves': 127},\n",
    "                    num_boost_round= 1200,\n",
    "                    early_stopping= 900,\n",
    "                    verbose_eval= 200,\n",
    "                    n_folds= 5\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1292343145.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\boomb\\AppData\\Local\\Temp\\ipykernel_25068\\1292343145.py\"\u001B[1;36m, line \u001B[1;32m58\u001B[0m\n\u001B[1;33m    def load_dataframes(message: str):\u001B[0m\n\u001B[1;37m                                      ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def read_datasets():\n",
    "    # load train, test\n",
    "    train = pd.read_parquet('train_data.parquet')\n",
    "    test = pd.read_parquet('test_data.parquet')\n",
    "    # print memory usage\n",
    "    print('train dataset mem usage:', mem_usage_gb(df=train, deep=True), 'GB')\n",
    "    print('test dataset mem usage:', mem_usage_gb(df=test, deep=True), 'GB')\n",
    "    # load true labels for train (target)\n",
    "    labels = pd.read_csv('train_labels.csv')\n",
    "    labels.drop(columns= ['customer_ID'], inplace= True)\n",
    "    # convert labels to numpy.array\n",
    "    labels = np.ravel(labels)\n",
    "    # get unique labels counts to see whether the dataset is imbalanced\n",
    "    counts, names = get_label_counts(labels)\n",
    "    # Creating plot\n",
    "    plt.pie(counts, labels = names)\n",
    "    plt.title('Counts of unique labels')\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    # return datasets\n",
    "    return train, test, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataset(train: pd.DataFrame, test: pd.DataFrame, prep_flag: bool):\n",
    "    # get numerical features\n",
    "    numerical = train.drop(columns= PREPROCESS.categorical).columns.to_list()\n",
    "    numerical.remove('customer_ID'); numerical.remove('S_2')\n",
    "\n",
    "    # preprocess train\n",
    "    # aggregate numerical columns for each customer by mean, std, min, max, last entry\n",
    "    # rows reduced from 5,5 mil to 458k\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[numerical].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_num_agg.drop(['customer_ID'], axis= 1, inplace= True)\n",
    "    # aggregate categoricals by count, last, number of unique entries\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[PREPROCESS.categorical].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_cat_agg.drop(['customer_ID'], axis= 1, inplace= True)\n",
    "    agg_cat_cols = train_cat_agg.columns.tolist()\n",
    "    train = pd.concat([train_num_agg, train_cat_agg], axis= 1)\n",
    "    del train_num_agg, train_cat_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # preprocess test\n",
    "    # essentially the same\n",
    "    # aggregate numerical columns for each customer by mean, std, min, max, last entry\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[numerical].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_num_agg.drop(['customer_ID'], axis= 1, inplace= True)\n",
    "    # aggregate categoricals by count, last, number of unique entries\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[PREPROCESS.categorical].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    test_cat_agg.drop(['customer_ID'], axis= 1, inplace= True)\n",
    "    test = pd.concat([test_num_agg, test_cat_agg], axis= 1)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    gc.collect()\n",
    "    # different encoding for different gradient boosting libraries\n",
    "    if prep_flag:\n",
    "        # prep for catboost.\n",
    "        # catboost accepts categorical features in raw form as string or integers\n",
    "        # no NaN's, float's can be present in categorical features\n",
    "        # change dtypes float to object, fill NaN in cat. columns with some string. I chose 'no_data'\n",
    "        # as I understood Catboost uses one-hot encoding automatically\n",
    "        print('preprocessing for Catboost')\n",
    "        nulls = train[agg_cat_cols].isna().sum(); types = train[agg_cat_cols].dtypes\n",
    "        cat_stats = pd.concat([nulls, types], axis=1).rename(columns={0: \"NaN_count\", 1: \"type\"})\n",
    "        cols_to_encode = cat_stats.loc[(cat_stats['NaN_count'] != 0) | (cat_stats['type'] == 'float16')].index.tolist()\n",
    "        for col in cols_to_encode:\n",
    "            train[col] = train[col].astype('object')\n",
    "            train[col] = train[col].map(str)\n",
    "        train[agg_cat_cols].fillna('no_data', inplace= True)\n",
    "        nulls = test[agg_cat_cols].isna().sum(); types = test[agg_cat_cols].dtypes\n",
    "        cat_stats = pd.concat([nulls, types], axis= 1).rename(columns= {0: \"NaN_count\", 1: \"type\"})\n",
    "        cols_to_encode = cat_stats.loc[(cat_stats['NaN_count'] != 0) | (cat_stats['type'] == 'float16')].index.tolist()\n",
    "        for col in cols_to_encode:\n",
    "            test[col] = test[col].astype('object')\n",
    "            test[col] = test[col].map(str)\n",
    "        test[agg_cat_cols].fillna('no_data', inplace=True)\n",
    "    else:\n",
    "        # else I one-hot encode manually\n",
    "        # unique values in categoricals of train and test differ\n",
    "        # one unique value in train appears, that does not in test\n",
    "        # I have to add a column of 0 manually to test for that value\n",
    "        # Initially I got the missing col by getting difference of sets of columns (train - test),\n",
    "        # but here the column is added manually for the sake of simplicity (I'm lazy)\n",
    "        print('Preprocessing for LGB / XGB')\n",
    "        train, test = one_hot(train, test)\n",
    "        idx = train.columns.get_loc('D_64_last_-1')\n",
    "        test.insert(loc=idx, column='D_64_last_-1', value=[0] * len(test))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_label_counts(labels: np.ndarray):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    names = []\n",
    "    for k, v in count_dict.items():\n",
    "        names.append(f'{k}, {round(v / len(labels), 2) * 100}%')\n",
    "        print(f'Count of {k}: ({v} / {len(labels)}), percentage = {round(v / len(labels), 4) * 100}%')\n",
    "    return counts, names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict_xgb(train_data: pd.DataFrame, test_data: pd.DataFrame, train_labels: np.ndarray, config: CONFIG):\n",
    "    total_predictions = np.zeros(test_data.shape[0])\n",
    "    oof_predictions = np.zeros(train_data.shape[0])\n",
    "    print(config)\n",
    "    skf = StratifiedKFold(n_splits=config.n_folds)\n",
    "    n = 1\n",
    "    for train_index, valid_index in skf.split(train_data, train_labels):\n",
    "        results = dict()\n",
    "        print('=' * 60)\n",
    "        print(f'Fold number: {n}')\n",
    "        print('=' * 60)\n",
    "        # Get counts of 0 and 1 in K-Fold labels\n",
    "        # to see that folds are indeed stratified (percentages of classes are preserved)\n",
    "        print('label counts in train')\n",
    "        get_label_counts(train_labels[train_index])\n",
    "        print('label counts in test(eval)')\n",
    "        get_label_counts(train_labels[valid_index])\n",
    "\n",
    "        d_train = xgb.DMatrix(train_data.iloc[train_index], train_labels[train_index])\n",
    "        d_valid = xgb.DMatrix(train_data.iloc[valid_index], train_labels[valid_index])\n",
    "\n",
    "        model = xgb.train(config.params,\n",
    "                          dtrain= d_train,\n",
    "                          num_boost_round= config.num_boost_round,\n",
    "                          evals= [(d_train, 'train'), (d_valid, 'valid')],\n",
    "                          custom_metric= xgb_amex_metric,\n",
    "                          early_stopping_rounds= config.early_stopping,\n",
    "                          evals_result= results,\n",
    "                          verbose_eval= config.verbose_eval,\n",
    "                          )\n",
    "\n",
    "        #=======================\n",
    "        # plot metrics\n",
    "        #=======================\n",
    "        epochs = len(results['valid']['amex_metric'])\n",
    "        x_axis = range(0, epochs)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, results['train']['amex_metric'], label='Train')\n",
    "        ax.plot(x_axis, results['valid']['amex_metric'], label='Valid')\n",
    "        ax.legend()\n",
    "        plt.ylabel('AMEX_METRIC')\n",
    "        plt.title('XGBoost AMEX')\n",
    "        plt.show()\n",
    "\n",
    "        # get fold predictions\n",
    "        valid_predictions = model.predict(d_valid)\n",
    "        oof_predictions[valid_index] = valid_predictions\n",
    "        # get test predictions\n",
    "        d_test = xgb.DMatrix(test_data)\n",
    "        fold_test_predictions = model.predict(d_test)\n",
    "        # adjust predictions according to n_folds\n",
    "        total_predictions += fold_test_predictions / config.n_folds\n",
    "        print('Fold model successfully trained. Predictions saved')\n",
    "        print(f'fold {n} score: {amex_metric(y_true=train_labels[valid_index], y_pred=valid_predictions)}')\n",
    "        n += 1\n",
    "    del d_train, d_valid, d_test\n",
    "    gc.collect()\n",
    "    print(f'Total out of folds score: {amex_metric(y_true=train_labels, y_pred=oof_predictions)}')\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    output = pd.DataFrame({'customer_ID': sample.customer_ID, 'prediction': total_predictions})\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict_lgbm(train_data: pd.DataFrame, test_data: pd.DataFrame, train_labels: np.ndarray, config: CONFIG):\n",
    "    total_predictions = np.zeros(test_data.shape[0])\n",
    "    oof_predictions = np.zeros(train_data.shape[0])\n",
    "    print(config)\n",
    "    skf = StratifiedKFold(n_splits=config.n_folds)\n",
    "    n = 1\n",
    "    for train_index, valid_index in skf.split(train_data, train_labels):\n",
    "        results = dict()\n",
    "        print('=' * 60)\n",
    "        print(f'Fold number: {n}')\n",
    "        print('=' * 60)\n",
    "        # Get counts of 0 and 1 in K-Fold labels\n",
    "        # to see that folds are indeed stratified (percentages of classes are preserved)\n",
    "        print('label counts in train')\n",
    "        get_label_counts(train_labels[train_index])\n",
    "        print('label counts in test(eval)')\n",
    "        get_label_counts(train_labels[valid_index])\n",
    "\n",
    "        d_train = lgb.Dataset(data=train_data.iloc[train_index], label=train_labels[train_index])\n",
    "        d_valid = lgb.Dataset(data=train_data.iloc[valid_index], label=train_labels[valid_index])\n",
    "        model = lgb.train(config.params,\n",
    "                          train_set = d_train,\n",
    "                          num_boost_round= config.num_boost_round,\n",
    "                          valid_sets= [d_train, d_valid],\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          feval=lgb_amex_metric,\n",
    "                          early_stopping_rounds= config.early_stopping,\n",
    "                          evals_result= results,\n",
    "                          verbose_eval= config.verbose_eval,\n",
    "                          )\n",
    "\n",
    "        #=======================\n",
    "        # plot metrics\n",
    "        #=======================\n",
    "        lgb.plot_metric(results)\n",
    "        plt.show()\n",
    "\n",
    "        # get fold predictions\n",
    "        valid_predictions = model.predict(d_valid)\n",
    "        oof_predictions[valid_index] = valid_predictions\n",
    "        # get test predictions\n",
    "        d_test = lgb.Dataset(test_data)\n",
    "        fold_test_predictions = model.predict(d_test)\n",
    "        # adjust predictions according to n_folds\n",
    "        total_predictions += fold_test_predictions / config.n_folds\n",
    "        print('Fold model successfully trained. Predictions saved')\n",
    "        print(f'fold {n} score: {amex_metric(y_true=train_labels[valid_index], y_pred=valid_predictions)}')\n",
    "        n += 1\n",
    "    del d_train, d_valid, d_test\n",
    "    gc.collect()\n",
    "    print(f'Total out of folds score: {amex_metric(y_true=train_labels, y_pred=oof_predictions)}')\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    output = pd.DataFrame({'customer_ID': sample.customer_ID, 'prediction': total_predictions})\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
